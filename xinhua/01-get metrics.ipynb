{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "830354a5-4085-4fcf-bc9d-64a72d38bf31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wu.xinh/.conda/envs/ptorch/lib/python3.10/site-packages/geopandas/_compat.py:111: UserWarning: The Shapely GEOS version (3.11.2-CAPI-1.17.2) is incompatible with the GEOS version PyGEOS was compiled with (3.10.4-CAPI-1.16.2). Conversions between both will be slow.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os, re\n",
    "import glob\n",
    "import zipfile\n",
    "import skmob\n",
    "from skmob.measures.individual import jump_lengths\n",
    "from skmob.measures.individual import radius_of_gyration\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9fc895-c92b-4616-9bf0-8e2ea9f41215",
   "metadata": {},
   "source": [
    "Change the sample_data_path below as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74676080-89df-4f7d-8e1a-fa323f02c771",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data_path = '/work/rwuirlab/Samples_MSAs'\n",
    "zip_files = glob.glob(os.path.join(sample_data_path, '*.zip'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08305a90-8c5c-4112-b20c-faed1014464d",
   "metadata": {},
   "source": [
    "you may not need to run the following if the processed data is ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15fd03b8-fe42-406a-a93b-883cdd6d4798",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_concat_csv_from_zip(zip_file_path,n_csvs=2000):\n",
    "    # Create an empty list to store dataframes\n",
    "    dfs = []\n",
    "    with zipfile.ZipFile(zip_file_path, 'r') as z:\n",
    "        # Get a list of all CSV files in the ZIP\n",
    "        csv_files = [f for f in z.namelist() if f.endswith('.csv')][:n_csvs]\n",
    "        for csv_file in csv_files:\n",
    "            # Open each CSV file and read it into a DataFrame\n",
    "            with z.open(csv_file) as f:\n",
    "                df = pd.read_csv(f,header=None)\n",
    "                df = df[[1,3,4,5,10]]\n",
    "                df.columns = ['user_id','lat','lon','acc','time']\n",
    "                df['time'] = pd.to_datetime(df['time'])\n",
    "                start_date = pd.Timestamp('2020-01-01 00:00:00')\n",
    "                end_date = pd.Timestamp('2020-06-30 23:59:59')\n",
    "                df = df[(df['time'] >= start_date) & (df['time'] <= end_date)]\n",
    "                \n",
    "                # Append the DataFrame to the list of DataFrames\n",
    "                dfs.append(df)\n",
    "    \n",
    "    # # Concatenate all DataFrames in the list into a single DataFrame\n",
    "    # # You can specify axis=0 for vertical stacking (rows), or axis=1 for horizontal stacking (columns)\n",
    "    # # Use ignore_index=True to reindex the new DataFrame\n",
    "    return pd.concat(dfs, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b147f028-d318-4bd4-a6ee-a57b8b3b37e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_daily_metrics(df):\n",
    "    df['date'] = df['time'].dt.date\n",
    "    # number of records each day each user\n",
    "    num_of_records_df = df.groupby(['user_id','date']).size().reset_index()\n",
    "    num_of_records_df.columns = ['user_id','date','num_of_records']\n",
    "    # temporal occupancy each day each user\n",
    "    df['half_hour_index'] = df['time'].dt.hour * 2 + df['time'].dt.minute // 30\n",
    "    df = df.drop_duplicates(['user_id','date','half_hour_index'])\n",
    "    temporal_occupancy_df = df.groupby(['user_id','date']).size().reset_index()\n",
    "    temporal_occupancy_df.columns = ['user_id','date','intra_day_temporal_occupancy']\n",
    "    # merge\n",
    "    merge_df = num_of_records_df.merge(temporal_occupancy_df,how='left',on=['user_id','date'])\n",
    "    return merge_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c1a79ff-70ba-437a-a156-0530f4fe10ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_longterm_metrics(df):\n",
    "    # high acc rate\n",
    "    high_acc_df = df.groupby('user_id')['acc'].apply(lambda x: (x < 100).mean()).reset_index()\n",
    "    high_acc_df.columns = ['user_id','acc_rate']\n",
    "    # radius of gyration\n",
    "    tdf = skmob.TrajDataFrame(df, latitude='lat', longitude='lon', datetime='time', user_id='user_id')\n",
    "    radius_of_gyration_df = radius_of_gyration(tdf,False)\n",
    "    radius_of_gyration_df.columns = ['user_id','radius_of_gyration']\n",
    "    # Euclidean distance mean\n",
    "    distance_mean_df = jump_lengths(tdf,False)\n",
    "    distance_mean_df['jump_lengths'] = distance_mean_df.jump_lengths.apply(lambda x: np.mean(x) if len(x) > 0 else np.nan)\n",
    "    distance_mean_df.columns = ['user_id','euclidean_distance_mean']\n",
    "    # merge\n",
    "    merge_df = pd.merge(pd.merge(high_acc_df, radius_of_gyration_df, on='user_id'), distance_mean_df, on='user_id')\n",
    "    return merge_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ba25aa7-ac8f-4cf8-b725-d6977c7f17d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_csv(dataframe, output_path, mode='a', header=False):\n",
    "    if not os.path.exists(output_path) or mode == 'w':\n",
    "        header = True\n",
    "    dataframe.to_csv(output_path, mode=mode, header=header, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3819b44-ec20-43a4-8a0c-3bc4a0d25605",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74dc702ad6c1442084ac6eafc4cd55b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# os.remove('Metrics/daily_metrics.csv')\n",
    "# os.remove('Metrics/longterm_metrics.csv')\n",
    "\n",
    "for zip_file in tqdm(zip_files):\n",
    "    if 'Sample_MSAs_Ind' in zip_file:\n",
    "        continue\n",
    "    match = re.search(r'/([^/]+)\\.zip$', zip_file)\n",
    "    MSA = match.group(1)\n",
    "    \n",
    "    df = read_concat_csv_from_zip(zip_file,2000)\n",
    "    daily_metrics_df = get_daily_metrics(df)\n",
    "    daily_metrics_df['MSA'] = MSA\n",
    "    write_to_csv(daily_metrics_df,'Metrics/daily_metrics.csv')\n",
    "    longterm_metrics_df = get_longterm_metrics(df)\n",
    "    longterm_metrics_df['MSA'] = MSA\n",
    "    write_to_csv(longterm_metrics_df,'Metrics/longterm_metrics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af85aee-b3a0-4fc0-a81c-4031aed4d7e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
